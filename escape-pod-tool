#!/usr/bin/perl
use strict;
use warnings;
use utf8;
use List::Util qw(max);
use File::Basename qw(dirname);

sub getConfig($);

my $config = undef;
my @podcasts = qw(escapepod podcastle pseudopod);
my $defaultPodcast = "escapepod";
my $okPodcasts = join "|", map {"--$_"} @podcasts;

my $podcastConfigs = {
  escapepod => getConfig({
    podcastName => 'Escape Pod',
    podcastWord => 'escapepod',
    podcastInitials => 'EP',
    firstUrlSuffix => '2005/05/12/ep001-imperial-by-jonathon-sullivan/',
    forumBoard => '1',
    useArchive => 'true',
    ignoreRating => 'false',
  }),
  podcastle => getConfig({
    podcastName => 'PodCastle',
    podcastWord => 'podcastle',
    podcastInitials => 'PC',
    firstUrlSuffix => '2007/07/05/hello-world/',
    forumBoard => '57',
    useArchive => 'true',
    ignoreRating => 'false',
  }),
  pseudopod => getConfig({
    podcastName => 'Pseudopod',
    podcastWord => 'pseudopod',
    podcastInitials => 'PP',
    firstUrlSuffix => '2006/08/11/pseudopod-001-bag-man/',
    forumBoard => '4',
    useArchive => 'true',
    ignoreRating => 'true',
  }),
};

sub getConfig($){
  my $base = shift;
  my $podcastWordsRe = " $$base{podcastWord} ";
  $podcastWordsRe =~ s/pod/ pod /i;
  $podcastWordsRe =~ s/\s+/\\s*/g;

  my $localDir = dirname(__FILE__);

  my $infoDir = "$localDir/$$base{podcastWord}";
  my $cacheDir = "$ENV{HOME}/.cache/$$base{podcastWord}";

  return {%$base,
    baseUrl => "http://$$base{podcastWord}.org",
    firstUrl => "http://$$base{podcastWord}.org/$$base{firstUrlSuffix}",
    forumUrl => "http://forum.escapeartists.net/index.php",
    podcastRegex => "(?:"
      . "(?:$podcastWordsRe|$$base{podcastInitials}|episode)"
      . "\\s*[\-,]?\\s*"
      . "(?:episode-?\\s*)?"
      . ")",

    infoDir => "$infoDir",
    urlOverridesFile => "$infoDir/url-overrides",
    cacheDir => "$cacheDir",

    mp3Dir => "$cacheDir/downloads",
    csvDelim => ';',
  };
}


sub parseHtml($$$$);
sub urlToFileName($);
sub fileNameToUrl($);
sub htmlCacheFile($);
sub deletedHtmlCacheFile($);
sub getCachedUrls();
sub ensureHtmlCache($);
sub attemptPutHtmlCache($);
sub checkHtmlFile($);
sub downloadHtmlFile($$;$);
sub readUrl($;$);
sub refetchHtmlCache($);
sub attemptGunzip($;$);
sub articleContainsText($$);
sub readHtmlCache($);
sub getMP3Url($);
sub browserLoadArticle($);
sub ensureArticleUrlCache();
sub getMissingArticleUrls($);
sub cleanUrl($);
sub getAllUrls();
sub getNextUrl($);
sub getAllUrlsFromArchive();
sub getUrlsFromArchiveHtml($);
sub getArticleUrls();
sub getUrlOverrides($);
sub readAttCache($);
sub readCacheFile($);
sub writeAttCache($$);
sub getAttCacheMtime($);
sub extraInfo();
sub personLinks();
sub fmtPersonLink($$);
sub crawlForum();
sub checkDownloadedFileSize($$);
sub downloadMP3File($$$$);
sub tagMP3File($$);
sub newMP3FileName($);

sub csv($);
sub cell($);

sub calMonthLtEq($$);
sub getLastCachedCalMonth();
sub getCurrentCalMonth();
sub getAllCalMonthsSince($);
sub arrEquals($$);
sub uniqArr(@);
sub globOne($);
sub run(@);
sub runMaybeQuiet($@);
sub tryrun(@);
sub tryrunMaybeQuiet($@);

my @ratings = (
  "G"     => 'OK for Kids',
  "PG"    => '10 and Up',
  "PG-13" => '13 and Up',
  "R"     => '17 and Up',
  "X"     => 'Erotica - NOT FOR KIDS',
);
my %ratingCategories = @ratings;
my @ratingOrder = @ratings[grep { !($_ & 1) } 0 .. $#ratings];

my $cmds = join "|", qw(
  --csv --url --wiki
  --mp3url --mp3filename
  --download --download-only --tag
  --refetch-url
);
my $usage = "Usage:
  $0 [PODCAST] [$cmds [EPNUM EPNUM ..]]
    Crawl BASE_URL and print information about each epiosde.
    HTML is fetched using curl/wget and cached in CACHE_DIR/html-cache,
    and is supplemented with the info in INFO_DIR

    --csv {default if no args specified}
      Print episode info in CSV with delim=CSV_DELIM
    --url
      Print the article url for each EPNUM
    --wiki
      Use article HTML pages and forums to generate output in wikipedia-list format.
      Prints to stdout and to the file CACHE_DIR/wiki-out, and copies to the clipboard.
        {runs 'cat CACHE_DIR/wiki-out | xsel -b'}
    --mp3url
      Print the download URL, parsed from the article HTML.
    --mp3filename
      Same as --mp3url, but only print the filename
    --download
      Get mp3url and download it with axel to MP3_DIR, then --tag it as below.
      Skip an EPNUM if the mp3filename
        or a file named \"###EPNUM TITLE.mp3\"
        is already present in the current dir.
    --download-only
      Same as --download, except do not --tag after downloading
    --tag
      Set tags and rename files.
      Gets mp3filename, assume it is in current dir.
      If mp3filename is not present, use the target name.

      Remove all id3 tags from the file, and set new tags.
      Uses 'id3v2' and 'mid3iconv'.
        title = \"TITLE\"
        author = \"AUTHOR [READER]\"
        tracknumber = \"EPNUM\"
        album = \"PODCAST_NAME\"
        date = \"DATE\"
      Rename the file from mp3filename => \"###EPNUM TITLE.mp3\"
      {with /s removed}

  $0 [PODCAST] --refetch-url
    Refresh article url cache

  PODCAST: one of \"$okPodcasts\", default is $defaultPodcast
    determines the following configurations
    --escapepod:
      PODCAST_NAME: $$podcastConfigs{escapepod}{podcastName}
      BASE_URL:     $$podcastConfigs{escapepod}{baseUrl}
      MP3_DIR:      $$podcastConfigs{escapepod}{mp3Dir}
      INFO_DIR:     $$podcastConfigs{escapepod}{infoDir}
      CACHE_DIR:    $$podcastConfigs{escapepod}{cacheDir}
    --podcastle:
      PODCAST_NAME: $$podcastConfigs{podcastle}{podcastName}
      BASE_URL:     $$podcastConfigs{podcastle}{baseUrl}
      MP3_DIR:      $$podcastConfigs{podcastle}{mp3Dir}
      INFO_DIR:     $$podcastConfigs{podcastle}{infoDir}
      CACHE_DIR:    $$podcastConfigs{podcastle}{cacheDir}

  EPNUM: podcast episode number. if none are specified, all are used.
         a range can be specified as:
         [LOW-HIGH]        LOW <= EPNUM <= HIGH
         [-HIGH]           EPNUM <= HIGH         (same as [1-HIGH])
         [LOW-]            EPNUM >= LOW          (same as [LOW-<max#>])
  TITLE: the title as fetched from the html and extra-info
  AUTHOR: the author as fetched from the html and extra-info
  READER: the reader as fetched from the html and extra-info
  DATE: the date as indicated in the article URL

";

sub main(@){
  my $podcast = shift if @_ > 0 and $_[0] =~ /^($okPodcasts)$/;
  $podcast = $defaultPodcast if not defined $podcast;
  $podcast =~ s/^--//;
  $config = $$podcastConfigs{$podcast};

  my $infoDir = $$config{infoDir};
  my $cacheDir = $$config{cacheDir};
  die "FATAL: missing INFO_DIR '$infoDir\n'" if not -d $infoDir;
  run "mkdir", "-p", $cacheDir if not -d $cacheDir;
  die "FATAL: missing CACHE_DIR '$cacheDir\n'" if not -d $cacheDir;

  my $cmd = shift;
  $cmd = '--csv' if not defined $cmd;
  die $usage if $cmd !~ /^($cmds)$/;

  if($cmd eq '--refetch-url' and @_ == 0){
    writeAttCache 'article-url-cache', {};
    ensureArticleUrlCache();
    exit 0;
  }

  my @nums;
  my @ranges;
  for my $num(@_){
    if($num =~ /^(\d+)([ab]?)$/){
      my $epNum = sprintf "%03d%s", $1+0, $2;
      push @nums, $epNum;
    }elsif($num =~ /^\[(\d+[ab]?)?-(\d+[ab]?)?\]$/){
      my ($low, $high) = ($1, $2);
      my ($epNumLow, $epNumHigh);
      if(not defined $low or $low eq ""){
        $epNumLow = undef;
      }else{
        $epNumLow = sprintf "%03d%s", $1+0, $2 if $low =~ /^(\d+)([ab]?)$/;
      }
      if(not defined $high or $high eq ""){
        $epNumHigh = undef;
      }else{
        $epNumHigh = sprintf "%03d%s", $1+0, $2 if $high =~ /^(\d+)([ab]?)$/;
      }
      push @ranges, [$epNumLow, $epNumHigh];
    }else{
      die $usage;
    }
  }

  ensureArticleUrlCache();
  my $articleUrls = readAttCache 'article-url-cache';
  my @allEpNums = sort keys %$articleUrls;
  my $maxEpNum = $allEpNums[-1];

  for my $range(@ranges){
    my $low = $$range[0];
    my $high = $$range[1];
    for my $epNum(@allEpNums){
      my $okLow = (not defined $low) || ($low le $epNum);
      my $okHigh = (not defined $high) || ($high ge $epNum);
      push @nums, $epNum if $okLow && $okHigh;
    }
  }

  my @epNums = @nums == 0 ? @allEpNums : @nums;

  die $usage if (grep {$_ !~ /^\d+[ab]?$/} @epNums) > 0;

  my %okNums = map {$_ => 1} @epNums;

  my $downloadOnly = 0;
  if($cmd eq "--download-only"){
    $downloadOnly = 1;
  }

  my $epInfo = {};
  my $extraInfo = extraInfo();
  for my $epNum(@epNums){
    my $url = $$articleUrls{$epNum};
    die "Missing article url for $epNum\n" if not defined $url;
    my $ex = $$extraInfo{$epNum};
    my $allowMissing = $downloadOnly;
    $$epInfo{$epNum} = sub{parseHtml $epNum, $url, $ex, $allowMissing};
  }

  my $exitCode = 0;

  if($cmd eq '--csv'){
    for my $epNum(sort keys %$epInfo){
      my $articleUrl = $$articleUrls{$epNum};
      ensureHtmlCache $articleUrl;
      my $info = &{$$epInfo{$epNum}}();
      print csv($info) . "\n";
    }
  }elsif($cmd eq '--url'){
    for my $epNum(sort keys %$epInfo){
      print "$$articleUrls{$epNum}\n";
    }
  }elsif($cmd eq "--wiki"){
    if(@nums == 0){
      for my $num(keys %$articleUrls){
        die $usage if $num !~ /^(\d+)([ab]?)$/;
        push @nums, sprintf "%03d%s", $1+0, $2;
      }
      @nums = sort @nums;
    }

    my $isAllEps = @nums == keys %$articleUrls;
    my $prefixFile = "$$config{infoDir}/wiki-prefix";
    my $suffixFile = "$$config{infoDir}/wiki-suffix";

    open OUT, "> $$config{cacheDir}/wiki-out";

    if($isAllEps and -e $prefixFile){
      my $prefix = `cat $prefixFile`;
      print $prefix;
      print OUT $prefix;
    }

    for my $epNum(reverse @nums){
      my $articleUrl = $$articleUrls{$epNum};
      my $personLinks = personLinks();
      if(ensureHtmlCache $articleUrl){
        my $info = &{$$epInfo{$epNum}}();

        my $rating = $$info{rating};
        $rating = "" if $rating eq "?";
        $rating =~ s/^\s*//;
        $rating =~ s/\s*$//;

        my $forumUrl = $$config{forumUrl};
        my $initials = $$config{podcastInitials};

        my $forum;
        if($$info{forum} ne "?"){
          $forum = "[$forumUrl?topic=$$info{forum} ${initials} Forum]";
        }else{
          $forum = "none";
        }

        die "Malformed episode number: $epNum\n" if $epNum !~ /^(\d+)([ab]?)$/;
        my $num = sprintf "%03d%s", $1+0, $2;

        my $note = '';
        $note = " $$info{note}" if defined $$info{note};
        my $fmt = ''
          . "|-\n|EP$num\n"
          . "|[$articleUrl $$info{title}]$note\n"
          . "|" . fmtPersonLink($$info{author}, $personLinks) . "\n"
          . "|" . fmtPersonLink($$info{reader}, $personLinks) . "\n"
          . ($$config{ignoreRating} eq "true" ? "" : "|$rating\n")
          . "|$$info{duration}\n"
          . "|$forum\n"
          ;
        print $fmt;
        print OUT $fmt;
      }else{
        print STDERR "   ERROR: $epNum\n";
      }
    }
    if($isAllEps and -e $suffixFile){
      my $suffix = `cat $suffixFile`;
      print $suffix;
      print OUT $suffix;
    }
    close OUT;
    run "cat $$config{cacheDir}/wiki-out | xsel -b";
  }elsif($cmd =~ /^(--mp3url|--mp3filename|--download|--download-only|--tag)/){
    my $mp3Dir = $$config{mp3Dir};
    run "mkdir", "-p", $mp3Dir if not -d $mp3Dir;
    die "$mp3Dir does not exist\n" if not -d $mp3Dir;
    chdir $mp3Dir;
    $ENV{PWD} = $mp3Dir;
    for my $epNum(sort keys %$epInfo){
      my $info = &{$$epInfo{$epNum}}();
      my $articleUrl = $$info{articleUrl};
      if(ensureHtmlCache $articleUrl){
        my $mp3Url = getMP3Url $articleUrl;
        my $mp3FileName = $1 if $mp3Url =~ /([^\/]*\.mp3)$/;
        if($cmd eq '--mp3url'){
          print "$mp3Url\n";
        }elsif($cmd eq '--mp3filename'){
          print "$mp3FileName\n";
        }elsif($cmd eq '--download'){
          downloadMP3File($mp3Url, $mp3FileName, $info, 1);
        }elsif($cmd eq '--download-only'){
          downloadMP3File($mp3Url, $mp3FileName, $info, 0);
        }elsif($cmd eq '--tag'){
          tagMP3File($mp3FileName, $info);
        }
      }else{
        print "   ERROR: $epNum\n";
        $exitCode = 1;
      }
    }
  }
  exit $exitCode;
}

sub parseHtml($$$$){
  my ($epNum, $url, $ex, $allowMissing) = @_;
  $url =~ s/^https:/http:/;

  my $html = readHtmlCache $url;

  my $info = {};
  $$info{number} = $epNum;
  $$info{articleUrl} = $url;
  $$info{date} = $1 if $url =~ /^$$config{baseUrl}\/(\d+\/\d+\/\d+)/;

  my @atts = qw(title author reader rating duration forum);

  my $mp3Url = getMP3Url $url;
  my $oldMP3FileName = $1 if $mp3Url =~ /([^\/]*\.mp3)$/;

  my $oldMP3File = "$$config{mp3Dir}/$oldMP3FileName";
  $oldMP3File = undef if not -f $oldMP3File;

  my $newMP3File = globOne "$$config{mp3Dir}/$epNum\\ *.mp3";

  my $mp3File;
  $mp3File = $oldMP3File if defined $oldMP3File;
  $mp3File = $newMP3File if defined $newMP3File;

  my $tag = "(?:<[^>]*>)";
  my $tagWs = "(?:\\s*<[^>]*>\\s*)";
  my $b = "(?:<\\/?\\s*(?:b|strong)[^>]*>)";
  my $span = "(?:<\\/?\\s*span(?:\\s[^>]*)?>)";
  my $anchor = "(?:<\\/?\\s*a(?:\\s[^>]*)?>)";
  my $ws = "[ \\t\\n]";
  my $readBy = "(?:narrated|read)$ws+(?:and produced )?by:?$ws+";

  $html =~ s/\xa0/ /g;
  $html =~ s/\xc2/ /g;
  $html =~ s/&nbsp;/ /g;
  $html =~ s/&#160;/ /g;
  $html =~ s/Â/ /g;

  my $htmlNoMeta = $html;
  $htmlNoMeta =~ s/<meta$ws*[^>]*>//g;

  if(defined $$ex{note}){
    $$info{note} = $$ex{note};
  }

  if(defined $$ex{title}){
    $$info{title} = $$ex{title};
  }elsif($html =~ /<title>(.*)<\/title>/i){
    my $title = $1;
    utf8::decode $title;
    $title =~ s/[:\.\-]\s*$$config{podcastName}\s*//g;
    $title =~ s/^\s*($$config{podcastRegex})\s*\d+[ab]?[:\-]?//i;
    $title =~ s/^\s*//;
    $title =~ s/\s*$//;
    $title =~ s/&#039;/'/g;
    $title =~ s/&#8217;/'/g;
    $title =~ s/&#8211;/-/g;
    $title =~ s/&#8230;/.../g;
    $title =~ s/&#8220;/"/g;
    $title =~ s/&#8221;/"/g;
    $title =~ s/&#8217;/'/g;
    $title =~ s/&amp;|&#038;/&/g;
    $title =~ s/’/'/g;
    $title =~ s/“/"/g;
    $title =~ s/”/"/g;
    $title =~ s/&quot;/"/g;
    $title =~ s/^\s*-\s*//g;
    $title =~ s/^$$config{podcastRegex}\s*\d+[ab]?(:\s*)?//;
    $title =~ s/\s*\|?\s*$$config{podcastName}\s*$//gi;
    $title =~ s/^\s*,\s*//;
    $title =~ s/^Giant\s*Episode\s*[:\-]?\s*//;
    $title =~ s/$$config{podcastName}.*$$config{podcastName}\s*\d+[ab]?:\s*//;
    utf8::encode $title;
    $$info{title} = $title;
  }

  my $isFlash;
  if(defined $$info{title} and $$info{title} =~ /^Flash (Contest|Extravaganza|Fiction)/){
    $isFlash = 1;
  }else{
    $isFlash = 0;
  }

  if(not $isFlash){
    if(defined $$ex{author}){
      $$info{author} = $$ex{author};
    }elsif($htmlNoMeta =~ /(?-i)$span?A(?i)uthor$span?$ws*:$ws*$span?$ws*$anchor?$span?$b?([^<(]+)/i){
      $$info{author} = $1;
    }elsif($htmlNoMeta =~ />by$ws*:?\s*$span?$b?$anchor?$b?([^<(]+)/i){
      $$info{author} = $1;
    }
  }else{
    my @authors;
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /$b$ws*by$ws+([^<(]*)$b/gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /$b$ws*by$ws+([^<(]*)</gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /<\/a>,$ws*by$ws+([^<(]*)</gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /(?<!narrated)$ws+by$ws+([^<(]*)</gi;
    }
    if(defined $$ex{author}){
      $$info{author} = $$ex{author};
    }else{
      $$info{author} = join ', ', @authors if @authors > 0;
    }
  }

  if(not $isFlash){
    if(defined $$ex{reader}){
      $$info{reader} = $$ex{reader};
    }elsif($htmlNoMeta =~ /$span?narrator?$span?$ws*:$ws*$span?$ws*$anchor?$span?$b?([^<(]+)/i){
      $$info{reader} = $1;
    }elsif($htmlNoMeta =~ /$readBy$span?$b?$anchor?$b?([^<(]+)/i){
      $$info{reader} = $1;
    }elsif($htmlNoMeta =~ /your\s*reader[^<]*(?:<[^>]+>\s*)+([^<]+)</i){
      $$info{reader} = $1;
    }elsif($htmlNoMeta =~ /^$tagWs*(?:narrators|narrator|$readBy|your\s*reader)(.*)$/mi){
      my $readerVal = $1;
      $readerVal =~ s/$tag//g;
      $readerVal =~ s/^\s*://;
      $readerVal =~ s/^\s*//;
      $readerVal =~ s/\s*$//;
      $$info{reader} = $readerVal;
    }
  }else{
    my @readers;
    if(@readers == 0){
      @readers = $htmlNoMeta =~ /$readBy$anchor?([^<]+)/gi;
    }
    if(@readers == 0){
      @readers = $htmlNoMeta =~ /\(narrator-? ([^)]+)\)/gi;
    }
    if(@readers == 0){
      if($htmlNoMeta =~ /your\s*reader[^<]*(?:<[^>]+>\s*)+([^<]+)</i){
        $$info{reader} = $1;
      }
    }
    if(defined $$ex{reader}){
      $$info{reader} = $$ex{reader};
    }else{
      $$info{reader} = join ', ', @readers if @readers > 0;
    }
  }

  if(defined $$info{reader} and $$info{reader} =~ /^\s*the\s*author\s*$/i){
    $$info{reader} = $$info{author};
  }

  $$ex{rating} = "?" if $$config{ignoreRating} eq "true";

  if(defined $$ex{rating}){
    $$info{rating} = $$ex{rating};
  }else{
    my $okRatings = join "|", reverse sort @ratingOrder;

    if(not defined $$info{rating}){
      if($html =~ /Rated\s*$b?\s*($okRatings)[ \t\n\.:\-<&]/){
        $$info{rating} = $1;
      }
    }

    if(not defined $$info{rating}){
      for my $rating(@ratingOrder){
        my $cat = $ratingCategories{$rating};
        if($html =~ /Rated\s*$b?\s*$cat[ \t\n\.:\-<&]/i){
          $$info{rating} = $rating;
        }
      }
    }

    if(not defined $$info{rating}){
      for my $rating(@ratingOrder){
        my $cat = $ratingCategories{$rating};
        die "$rating" if not defined $cat;
        if($html =~ /<meta property="article:section" content="$cat"\s*\/>/){
          $$info{rating} = $rating;
          last;
        }elsif($html =~ /<a[^>]*>$cat<\/a>/){
          $$info{rating} = $rating;
          last;
        }
      }
    }
  }

  my $durCache = readAttCache 'duration-cache';
  if(defined $$durCache{$epNum}){
    $$info{duration} = $$durCache{$epNum};
  }elsif(defined $mp3File){
    open CMD, "-|", "duration", "-n", $mp3File;
    my $dur = <CMD>;
    close CMD;
    chomp $dur;
    if($dur =~ /^(\d+:)+\d+$/){
      $$info{duration} = $dur;
      $$durCache{$epNum} = $dur;
      writeAttCache 'duration-cache', $durCache;
    }
  }elsif($html =~ /<span[^>]*>\s*\[\s*((?:\d+:)+\d+)\s*\]\s*<\/span>/){
    $$info{duration} = $1;
  }

  my $forumCache = readAttCache 'forum-cache';
  if(defined $$ex{forum}){
    my $prev = $$forumCache{$epNum};
    if(not defined $prev or $prev ne $$ex{forum}){
      $$forumCache{$epNum} = $$ex{forum};
    }
  }
  if(not defined $$forumCache{$epNum}){
    my $forumCacheMtime = getAttCacheMtime('forum-cache');
    $forumCacheMtime = 0 if not defined $forumCacheMtime;
    my $diff = time - $forumCacheMtime;
    if($diff >= 0 and $diff < 24*60*60){
      print STDERR "forum cache is less than a day old, not updating\n";
    }else{
      print STDERR "updating forum cache\n";
      my $epArticles = crawlForum;
      $forumCache = {};
      for my $ep(sort keys %$epArticles){
        $$forumCache{$ep} = $$epArticles{$ep};
      }
      writeAttCache 'forum-cache', $forumCache;
    }
  }
  if(defined $$forumCache{$epNum}){
    $$info{forum} = $$forumCache{$epNum};
  }

  for my $att(@atts){
    if(not defined $$info{$att} or $$info{$att} =~ /^\s*$/){
      print STDERR "Missing '$att' for $epNum  -  $url\n";
      if($allowMissing){
        $$info{$att} = "?";
      }else{
        exit 1;
      }
    }
  }

  return $info;
}

sub urlToFileName($){
  my $url = shift;
  my $name = $url;
  $name =~ s/^https:/http:/;
  $name =~ s/^\Q$$config{baseUrl}\E//;
  $name =~ s/^\///;
  $name =~ s/\/$//;
  $name =~ s/\//%%/g;
  return $name;
}

sub fileNameToUrl($){
  my $name = shift;
  $name =~ s/%%/\//g;
  return "$$config{baseUrl}/$name/";
}

sub htmlCacheFile($){
  my $url = shift;
  my $name = urlToFileName $url;

  my $htmlCacheDir = "$$config{cacheDir}/html-cache";
  run "mkdir", "-p", $htmlCacheDir if not -d $htmlCacheDir;
  die "html cache dir missing: '$htmlCacheDir'\n" if not -d $htmlCacheDir;

  return "$htmlCacheDir/$name";
}
sub deletedHtmlCacheFile($){
  my $url = shift;
  my $name = urlToFileName $url;

  my $deletedHtmlCacheDir = "$$config{infoDir}/deleted-html-cache";
  if(not -d $deletedHtmlCacheDir){
    return undef;
  }
  my $deletedHtmlCacheFile = "$deletedHtmlCacheDir/$name";
  if(not -f $deletedHtmlCacheFile){
    return undef;
  }
  return $deletedHtmlCacheFile;
}

sub getCachedUrls(){
  my $htmlCacheDir = "$$config{cacheDir}/html-cache";
  my @files = `ls "$htmlCacheDir" 2>/dev/null`;
  my @urls;
  for my $file(@files){
    $file =~ s/.*\///;
    $file =~ s/\n$//;
    my $url = fileNameToUrl $file;
    push @urls, $url;
  }
  return @urls;
}

sub ensureHtmlCache($){
  my $url = shift;
  my $cacheFile = htmlCacheFile $url;

  my $attempts;
  return 1 if checkHtmlFile $cacheFile;

  my $deletedHtmlCacheFile = deletedHtmlCacheFile $url;
  if(defined $deletedHtmlCacheFile){
    run "cp", "-ar", $deletedHtmlCacheFile, $cacheFile;
    return 1 if checkHtmlFile $cacheFile;
  }

  $attempts = 10;
  while($attempts > 0 and not checkHtmlFile $cacheFile){
    $attempts--;
    attemptPutHtmlCache $url;
  }
  if(not checkHtmlFile $cacheFile){
    browserLoadArticle $url;
  }
  $attempts = 10;
  while($attempts > 0 and not checkHtmlFile $cacheFile){
    $attempts--;
    attemptPutHtmlCache $url;
  }

  return checkHtmlFile $cacheFile;
}

sub attemptPutHtmlCache($){
  my $url = shift;
  my $cacheFile = htmlCacheFile $url;

  downloadHtmlFile $url, $cacheFile;
}

sub checkHtmlFile($){
  my ($file) = @_;
  if(-f $file){
    open FH, "< $file" or die "Could not read $file\n";
    my $content = join '', <FH>;
    close FH;
    if($content =~ /<html/){
      return 1;
    }
  }
  return 0;
}

sub downloadHtmlFile($$;$){
  my ($url, $file, $quiet) = (@_, 0);

  return 1 if checkHtmlFile $file;

  runMaybeQuiet $quiet, "curl -k -L \"$url\" -o \"$file\" 2>/dev/null";
  return 1 if checkHtmlFile $file;
  attemptGunzip($file, $quiet);
  return 1 if checkHtmlFile $file;

  runMaybeQuiet $quiet, "wget \"$url\" -O \"$file\" 2>/dev/null";
  return 1 if checkHtmlFile $file;
  attemptGunzip($file, $quiet);
  return 1 if checkHtmlFile $file;

  runMaybeQuiet $quiet, "rm", "-f", $file;
  return 0;
}

sub readUrl($;$){
  my ($url, $quiet) = (@_, 0);
  my $file = "/tmp/escape-pod-tool-" . time . ".html";
  system "rm", "-f", $file;
  if(downloadHtmlFile $url, $file, $quiet){
    my $content = `cat $file`;
    system "rm", "-f", $file;
    return $content;
  }
  return undef;
}

sub refetchHtmlCache($){
  my $url = shift;

  my $file = htmlCacheFile $url;
  my $tmp = "$file.tmp";

  if(-e $file){
    run "mv", $file, $tmp;
    if(ensureHtmlCache $url){
      system "rm", $tmp;
    }else{
      system "mv", $tmp, $file;
    }
  }else{
    ensureHtmlCache $url;
  }
}

sub attemptGunzip($;$){
  my ($file, $quiet) = @_;
  if(-e $file){
    runMaybeQuiet $quiet, "mv", $file, "$file.gz";
    tryrunMaybeQuiet $quiet, "gunzip", "$file.gz";
  }
  if(-e "$file.gz"){
    runMaybeQuiet $quiet, "rm", "-f", "$file.gz";
  }
}

sub articleContainsText($$){
  my ($url, $text) = @_;
  my $html = readHtmlCache $url;
  return $html =~ /\Q$text\E/;
}

sub readHtmlCache($){
  my $url = shift;
  my $cacheFile = htmlCacheFile $url;
  die "Missing html file for $url\n" if not checkHtmlFile $cacheFile;
  open FH, "< $cacheFile" or die "failed to read $cacheFile\n";
  my @lines = <FH>;
  close FH;
  return join '', @lines;
}

sub browserLoadArticle($){
  my $articleUrl = shift;
  system "uzbl $articleUrl >/dev/null 2>/dev/null &";
  sleep 5;
  system "pkill", "-f", "uzbl.*$articleUrl";
}

sub ensureArticleUrlCache(){
  my $cache = readAttCache 'article-url-cache';

  my $deletedCacheFile = "$$config{infoDir}/deleted-article-url-cache";
  my $deletedCache = {};
  if(-f $deletedCacheFile){
    $deletedCache = readCacheFile $deletedCacheFile;
  }

  $cache = {%$cache, %$deletedCache};

  my @missing = getMissingArticleUrls $cache;
  if(keys %$cache == 0 or @missing > 0){
    $cache = getArticleUrls();
    $cache = {%$cache, %$deletedCache};
    @missing = getMissingArticleUrls $cache;
  }
  if(@missing > 0){
    print STDERR "Missing articles:\n" . join("\n", @missing) . "\n";
  }
  if(keys %$cache == 0 or @missing > 0){
    die "article url cache is incomplete\n";
  }
  writeAttCache 'article-url-cache', $cache;
  return $cache;
}

sub getMissingArticleUrls($){
  my $cache = shift;
  my @epNums = keys %$cache;
  return () if @epNums == 0;
  my @numericEpNums = map {s/[^0-9]+//g; 0+$_} @epNums;
  my %okNumericEpNums = map {$_ => 1} @numericEpNums;
  my $max = max @numericEpNums;
  my @expectedNums = (1..$max);
  my @missing = grep {not defined $okNumericEpNums{$_}} @expectedNums;

  my @missingEpisodes = `cat $$config{infoDir}/missing-episodes 2>/dev/null`;
  chomp foreach @missingEpisodes;
  my %expectedMissing = map {$_ => 1} @missingEpisodes;
  @missing = grep {not defined $expectedMissing{$_}} @missing;

  return @missing;
}

sub cleanUrl($){
  my ($url) = @_;
  if(not defined $url){
    return undef;
  }

  #always use all-lowercase URLs
  $url = lc $url;

  #trim leading non-URL chars
  $url =~ s/^[^a-zA-Z0-9\-_\.~\!\*'\(\);:@&=+\$,\/?%#\[\]]+//;

  #trim trailing non-URL chars
  $url =~ s/[^a-zA-Z0-9\-_\.~\!\*'\(\);:@&=+\$,\/?%#\[\]]+$//;

  #always end URL with a single slash, or an anchor ref
  if($url !~ /#[^\/]*/){
    $url =~ s/\/+$//;
    $url = "$url/";
  }

  return $url;
}

sub getAllUrls(){
  if($$config{useArchive} eq 'true'){
    return getAllUrlsFromArchive();
  }
  my $url = $$config{firstUrl};
  my @urls;

  print STDERR "fetching all posts from $$config{baseUrl}\n";
  while(defined $url){
    print STDERR ".";
    attemptPutHtmlCache $url;
    push @urls, $url;
    my $nextUrl = getNextUrl($url);
    if(not defined $nextUrl){
      print "\nrefetching $url\n";
      refetchHtmlCache $url;
      $nextUrl = getNextUrl($url);
    }
    $url = $nextUrl;
  }
  print STDERR "\n";
  return @urls;
}

sub getNextUrl($){
  my $url = shift;
  my $html = readHtmlCache $url;
  my @anchors = $html =~ /<a [^>]+>/g;
  for my $anchor(@anchors){
    if($anchor =~ /rel="next"/ and $anchor =~ /href="([^"]+)"/){
      my $anchorUrl = $1;
      $anchorUrl = cleanUrl($anchorUrl);
      return $anchorUrl;
    }
  }
  return undef;
}

sub getAllUrlsFromArchive(){
  my $firstUrl = $$config{firstUrl};
  if($firstUrl !~ /^$$config{baseUrl}\/(\d\d\d\d)\/(\d\d)/){
    die "Error finding first archive month in '$firstUrl'\n";
  }
  my ($firstYear, $firstMonth) = ($1, $2);
  my $calMonthCache = getLastCachedCalMonth();
  my @urls;
  for my $calMonth(getAllCalMonthsSince([$firstYear, $firstMonth])){
    my ($year, $month) = @$calMonth;
    print "processing: $year/$month\n";
    my $curPage = 1;
    my $hasNextPage = 1;
    my $monthUrl = "$$config{baseUrl}/$year/$month";
    while($hasNextPage){
      my $url = $monthUrl;
      $url = "$url/page/$curPage" if $curPage > 1;
      if(defined $calMonthCache and calMonthLtEq($calMonthCache, $calMonth)){
        print "\nat or after last cached month, refetching\n";
        refetchHtmlCache $url;
      }else{
        attemptPutHtmlCache $url;
      }
      my @archiveUrls = getUrlsFromArchiveHtml $url;
      @urls = (@urls, @archiveUrls);
      $curPage++;

      my $nextUrl = "$monthUrl/page/$curPage/";
      $hasNextPage = articleContainsText $url, $nextUrl;
    }
  }

  my %uniqUrls = map {$_ => 1} @urls;
  @urls = sort keys %uniqUrls;

  print "\n\nfetching articles\n";
  for my $url(@urls){
    attemptPutHtmlCache $url;
  }
  return @urls;
}

sub getUrlsFromArchiveHtml($){
  my $url = shift;
  my $html = readHtmlCache $url;
  my @anchors = $html =~ /<a [^>]+>/g;
  my @urls;
  for my $anchor(@anchors){
    if($anchor =~ /href="($$config{baseUrl}\/\d+\/\d+\/\d+\/[^"]+)"/){
      my $anchorUrl = $1;
      $anchorUrl = cleanUrl($anchorUrl);
      push @urls, $anchorUrl;
    }
  }
  return sort @urls;
}

sub getArticleUrls(){
  my $urlOverrides = getUrlOverrides($$config{urlOverridesFile});
  my $articleUrls = {};
  my @errors;
  for my $url(getAllUrls()){
    my $num = undef;
    my $skip = 0;
    my $html = readHtmlCache $url;

    my $title = $1 if $html =~ /<title[^>]*>([^<]*)<\/title>/;

    my $titleNum;
    my $regex = "(?:$$config{podcastRegex})-?(\\d+[a-b]?)";
    if(defined $title and $title =~ /$regex/i){
      $titleNum = $1;
    }elsif($url =~ /$regex/){
      $titleNum = $1;
    }

    if($url !~ /(#comments|#more-\d+)$/){
      $num = $titleNum;
    }

    if(defined $$urlOverrides{$url}){
      if($$urlOverrides{$url} =~ /^(skip)$/i){
        $skip = 1;
      }elsif($$urlOverrides{$url} =~ /^(\d+[ab]?)$/){
        $num = $1;
      }
    }

    if(defined $num and not $skip){
      die $usage if $num !~ /^(\d+)([ab]?)$/;
      my $epNum = sprintf "%03d%s", $1+0, $2;
      my $prevUrl = $$articleUrls{$epNum};
      if(defined $prevUrl and $prevUrl ne $url){
        push @errors, "mismatched URL for $epNum:\nold: '$prevUrl'\nnew: '$url'\n";
      }
      $$articleUrls{$epNum} = $url;
    }
  }
  die @errors if @errors > 0;
  return $articleUrls;
}

sub getUrlOverrides($){
  my $file = shift;
  my @lines = `cat $file 2>/dev/null`;
  my $overrides = {};
  for my $line(@lines){
    if($line =~ /^(\d+[ab]?|skip) (.*)$/){
      $$overrides{$2} = $1;
    }
  }
  return $overrides;
}

sub readAttCache($){
  my $name = shift;
  my $cacheFile = "$$config{cacheDir}/$name";
  return readCacheFile $cacheFile;
}
sub readCacheFile($){
  my $cacheFile = shift;
  my @lines = `cat $cacheFile 2>/dev/null`;
  my $cache = {};
  for my $line(@lines){
    if($line =~ /^(\d+[ab]?)\s+(.*)$/){
      $$cache{$1} = $2;
    }
  }
  return $cache;
}

sub writeAttCache($$){
  my ($name, $cache) = @_;
  my $cacheFile = "$$config{cacheDir}/$name";
  my $s = '';
  open FH, "> $cacheFile" or die "Couldnt write to $cacheFile\n";
  for my $epNum(sort {$a cmp $b} keys %$cache){
    die "Malformed episode number: $epNum\n" if $epNum !~ /^(\d+)([ab]?)$/;
    print FH sprintf "%03d%s %s\n", $1, $2, $$cache{$epNum};
  }
  close FH;
}

sub getAttCacheMtime($){
  my ($name) = @_;
  my $cacheFile = "$$config{cacheDir}/$name";
  if(-f $cacheFile){
    my @stat = stat $cacheFile;
    return $stat[9];
  }else{
    return undef;
  }
}

sub extraInfo(){
  my $file = "$$config{infoDir}/extra-info";
  my $extraInfo = {};
  for my $line(`cat $file 2>/dev/null`){
    $line =~ s/#.*//;
    next if $line =~ /^\s*$/;
    if($line =~ /^(\d\d\d[ab]?) (title|author|reader|rating|forum|note) (.*)$/){
      $$extraInfo{$1}{$2} = $3;
    }else{
      die "malformed extra-info line: $line";
    }
  }
  return $extraInfo;
}

sub personLinks(){
  my $file = "$$config{infoDir}/person-links";
  my $personLinks = {};
  for my $line(`cat $file 2>/dev/null`){
    $line =~ s/#.*//;
    next if $line =~ /^\s*$/;
    if($line =~ /^\s*(\S.*\S)\s*=>\s*(\S.*\S)$/){
      my ($person, $link) = ($1, $2);
      $$personLinks{$person} = $link;
    }else{
      die "malformed person-links line: $line";
    }
  }
  return $personLinks;
}

sub fmtPersonLink($$){
  my ($person, $links) = @_;
  return $person if $person =~ /[\[\]]/;

  $person =~ s/\s*\.\s*Music\s*by\s*$//i;
  $person =~ s/&#8217;/'/g;
  $person =~ s/&amp;/&/g;
  my @people = split /(?:(?:\s+|,)and\s+)|(?:\s*[&,]\s*)/, $person;
  @people = grep {defined $_ and $_ =~ /\w/} @people;
  @people = grep {not $_ =~ /^\s*e-?mail[ :-]*$/} @people;
  @people = uniqArr @people;

  my @fmtPeople;
  for my $p(@people){
    $p =~ s/\s*\.?\s*$//;
    $p =~ s/ of the$//;
    $p =~ s/ of$//;
    $p =~ s/^author //;
    $p =~ s/^and //;
    $p =~ s/^\s*:\s*//;
    $p =~ s/;\s*courtesy\s*$//;
    $p =~ s/^\s*//;
    $p =~ s/\s*$//;
    my $link = $$links{$p};
    my $fmt;
    if(not defined $link){
      $fmt = "[[$p]]";
    }elsif($link eq "NONE"){
      $fmt = "$p";
    }elsif($link =~ /^[ a-zA-Z0-9_\-]+$/){
      $fmt = "[[$p ($link)|$p]]";
    }else{
      $fmt = "[$link $p]";
    }
    push @fmtPeople, $fmt;
  }

  return join ", ", @fmtPeople;
}

sub crawlForum(){
  my $page = 0;
  my $epTopics = {};
  my $prevEps = [];
  while($page < 50){
    my $board = "$$config{forumBoard}." . (20*$page);
    my $url = "$$config{forumUrl}?board=$board";
    my $html = readUrl $url, 1;
    my $pageEps = [];
    my $forumAnchor = "";
    my %eps;
    my %topics;
    while($html =~ /<a\s*href="[^"]*topic=(\d+\.\d+)">([^<]+)/gi){
      my ($topic, $contents) = ($1, $2);
      $topics{$topic} = $contents;
    }
    for my $topic(sort keys %topics){
      my $contents = $topics{$topic};
      my $nameRE = $$config{podcastRegex};
      my $numRE = '\d+[ab]?';
      my $andRE = '(?:-|&|&amp;|/)';
      if($contents =~ /^\s*$nameRE\s*$numRE\*/i){
        next; #skip topics with stars after the episode number, e.g.: EP2**8
      }elsif($contents =~ /^\s*$nameRE\s*($numRE)\s*(?:$andRE)\s*(?:$nameRE)?($numRE)/i){
        my ($epNum1, $epNum2) = ($1, $2);
        $eps{$epNum1} = $topic;
        $eps{$epNum2} = $topic;
      }elsif($contents =~ /^\s*$nameRE\s*($numRE)/i){
        my $epNum = $1;
        $eps{$epNum} = $topic;
      }
    }

    for my $ep(keys %eps){
      my $topic = $eps{$ep};
      die "malformed forum link\n" if $ep !~ /^(\d+)([ab]?)$/;
      my $epNum = sprintf "%03d%s", $1+0, $2;
      $$epTopics{$epNum} = $topic;
      push @$pageEps, $epNum;
    }
    print "@$pageEps\n";
    if(arrEquals $pageEps, $prevEps){
      print "REPEAT AT PAGE $page\n";
      last;
    }
    $prevEps = $pageEps;
    $page++;
  }
  return $epTopics;
}

sub getMP3Url($){
  my $articleUrl = shift;
  my $html = readHtmlCache $articleUrl;
  if($html !~ /href="([^"]*\.mp3)"/){
    die "Could not read mp3 url for $articleUrl\n";
  }
  return $1;
}

sub checkDownloadedFileSize($$){
  my ($remoteUrl, $localFile) = @_;
  if(not -f $localFile){
    die "ERROR: download failed for $remoteUrl\n$localFile does not exist\n";
  }

  my ($expectedBytes, $actualBytes);

  my $wgetSpider = `wget --spider '$remoteUrl' 2>&1`;
  if($wgetSpider =~ /^Length:\s*(\d+)\s+/m){
    $expectedBytes = $1;
  }

  my @stat = stat $localFile;
  $actualBytes = $stat[7];

  if(not defined $expectedBytes or $expectedBytes !~ /^\d+$/ or $expectedBytes == 0){
    die "ERROR: could not fetch remote file size of: $remoteUrl\n";
  }elsif(not defined $actualBytes or $actualBytes !~ /^\d+$/ or $actualBytes == 0){
    die "ERROR: could not find local file size of: $localFile\n";
  }elsif($actualBytes != $expectedBytes){
    die "ERROR: mismatched local vs remote file size:\n"
      . "$expectedBytes => $remoteUrl\n"
      . "$actualBytes => $localFile\n"
      ;
  }
}

sub downloadMP3File($$$$){
  my ($mp3Url, $mp3FileName, $ep, $tagFile) = @_;
  my $newFileName = newMP3FileName $ep;
  if(-e $mp3FileName or -e $newFileName){
    print "  skipping $$ep{number}..\n";
  }else{
    run "axel", "--alternate", $mp3Url;
    checkDownloadedFileSize($mp3Url, $mp3FileName);
    tagMP3File $mp3FileName, $ep if $tagFile;
  }
}
sub tagMP3File($$){
  my ($mp3FileName, $ep) = @_;
  my $oldFileName = $mp3FileName;
  my $newFileName = newMP3FileName $ep;

  if(not -e $oldFileName and not -e $newFileName){
    print "   ERROR: missing $oldFileName or $newFileName\n";
    return;
  }
  $oldFileName = $newFileName if not -e $oldFileName;

  run "id3v2", "--delete-all", $oldFileName;

  my $num = 0;
  $num += $1 if $$ep{number} =~ /(\d+)/;

  run "id3v2",
    "--TIT2", $$ep{title},
    "--TPE1", $$ep{author},
    "--TPE2", $$ep{reader},
    "--TRCK", $num,
    "--TALB", $$config{podcastName},
    "--TYER", $$ep{date},
    $oldFileName;

  run "mid3iconv", $oldFileName;

  if($oldFileName ne $newFileName){
    run "mv", "--no-clobber", $oldFileName, $newFileName;
  }
}
sub newMP3FileName($){
  my $ep = shift;
  my $newFileName = "$$ep{number} $$ep{title}.mp3";
  $newFileName =~ s/\//_/g;
  return $newFileName;
}

sub csv($){
  my $ep = shift;
  my @cols = (
    cell $$ep{number},
    cell $$ep{title},
    cell $$ep{author},
    cell $$ep{reader},
    cell $$ep{date},
    cell $$ep{articleUrl},
  );
  return join ($$config{csvDelim}, @cols);
}
sub cell($){
  my $cell = shift;
  $cell =~ s/\&amp;/\&/g;
  if($cell =~ /\Q$$config{csvDelim}\E/ or $cell =~ /\Q"\E/){
    $cell =~ s/"/""/g;
    $cell = "\"$cell\"";
    return $cell;
  }else{
    return $cell;
  }
}

sub calMonthLtEq($$){
  my ($oneCalMonth, $twoCalMonth) = @_;
  my ($oneYear, $oneMonth) = @$oneCalMonth;
  my ($twoYear, $twoMonth) = @$twoCalMonth;
  if($oneYear < $twoYear or ($oneYear == $twoYear and $oneMonth <= $twoMonth)){
    return 1;
  }else{
    return 0;
  }
}

sub getLastCachedCalMonth(){
  my @urls = getCachedUrls();
  my $maxCalMonth = undef;
  for my $url(@urls){
    if($url =~ /^\Q$$config{baseUrl}\E\/(\d\d\d\d)\/(\d\d)\/$/){
      my $calMonth = [$1, $2];
      if(not defined $maxCalMonth or calMonthLtEq($maxCalMonth, $calMonth)){
        $maxCalMonth = $calMonth;
      }
    }
  }
  return $maxCalMonth;
}

sub getCurrentCalMonth(){
  my @time = localtime;
  return [1900 + $time[5], sprintf "%02d", 1+$time[4]];
}
sub getAllCalMonthsSince($){
  my $calMonth = shift;
  my $curCalMonth = getCurrentCalMonth();

  my @months;
  while(calMonthLtEq($calMonth, $curCalMonth)){
    my ($year, $month) = @$calMonth;
    push @months, [sprintf("%04d", $year), sprintf("%02d", $month)];
    $month++;
    if($month > 12){
      $month = 1;
      $year++;
    }
    $calMonth = [$year, $month];
  }
  return @months;
}

sub uniqArr(@){
  my %seen;
  my @newArr;
  for my $elem(@_){
    push @newArr, $elem if not defined $seen{$elem};
    $seen{$elem} = 1;
  }
  return @newArr;
}
sub arrEquals($$){
  my ($arr1, $arr2) = @_;
  return 0 if not defined $arr1 or not defined $arr2;
  return 0 if @$arr1 != @$arr2;
  for(my $i=0; $i<@$arr1; $i++){
    return 0 if $$arr1[$i] ne $$arr2[$i];
  }
  return 1;
}

sub globOne($){
  my ($pattern) = @_;
  my @files = glob $pattern;
  if(@files > 1){
    die "error: globOne() returned multiple files: @files\n";
  }elsif(@files == 1){
    return $files[0];
  }else{
    return undef;
  }
}

sub run(@){
  runMaybeQuiet 0, @_;
}
sub runMaybeQuiet($@){
  my $quiet = shift;
  tryrunMaybeQuiet $quiet, @_;
  die "error running @_\n" if $? != 0;
}
sub tryrun(@){
  tryrunMaybeQuiet 0, @_;
}
sub tryrunMaybeQuiet($@){
  my $quiet = shift;
  print "@_\n" if not $quiet;
  system @_;
}

&main(@ARGV);
